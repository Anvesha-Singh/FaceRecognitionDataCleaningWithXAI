{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition model with Adaptive Noise Label Canceling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from FaceRecognitionModel import FaceRecognitionRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class LFWDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "lfw_path = r'C:\\Users\\Anvesha\\Documents\\Assignments\\AWS\\Implementation\\lfw_data\\aug'\n",
    "input_size = 4096  # Adjusted for 64x64 grayscale image flattened\n",
    "\n",
    "# Loading function\n",
    "def load_lfw_data():\n",
    "    X, y = [], []\n",
    "    for person_dir in os.listdir(lfw_path):\n",
    "        person_path = os.path.join(lfw_path, person_dir)\n",
    "        for image_file in os.listdir(person_path):\n",
    "            if image_file.endswith('.jpg'):\n",
    "                image_path = os.path.join(person_path, image_file)\n",
    "                image = Image.open(image_path).convert('L').resize((64, 64))\n",
    "                X.append(np.array(image).flatten())\n",
    "                y.append(person_dir)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load the data\n",
    "X, y = load_lfw_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data while maintaining class distribution\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_encoded, test_size=0.4, stratify=y_encoded)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_noisy_labels(X, y, num_epochs=5, batch_size=32):\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Create a DataLoader for the dataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Set the loss function and optimizer\n",
    "    hidden_size = 256\n",
    "    num_layers = 1\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    model = FaceRecognitionRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    # Train the model on the input data\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.view(images.size(0), -1, input_size), labels  # Reshape for LSTM\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Calculate the loss for each training sample\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Calculate the losses\n",
    "        losses = []\n",
    "        for images, labels in data_loader:\n",
    "            images = images.view(images.size(0), -1, input_size)  # Reshape for LSTM\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # Convert losses to a numpy array\n",
    "    losses = np.array(losses)\n",
    "\n",
    "    # Compute the mean and standard deviation of the loss distribution\n",
    "    loss_mean = np.mean(losses)\n",
    "    loss_std = np.std(losses)\n",
    "\n",
    "    # Define the threshold for identifying noisy labels\n",
    "    threshold = loss_mean + 2 * loss_std\n",
    "\n",
    "    # Identify noisy samples based on the threshold\n",
    "    noisy_indices = np.where(losses > threshold)[0]\n",
    "\n",
    "    # Create the cleaned training set\n",
    "    X_cleaned = np.delete(X, noisy_indices, axis=0)\n",
    "    y_cleaned = np.delete(y, noisy_indices)\n",
    "\n",
    "    return X_cleaned, y_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and clean noisy labels\n",
    "X_train_denoised, y_train_denoised = detect_noisy_labels(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels \n",
    "unique_classes = np.unique(y_train_denoised)\n",
    "if unique_classes[0] != 0 or len(unique_classes) != (unique_classes[-1] + 1):\n",
    "    label_map = {old_label: new_label for new_label, old_label in enumerate(unique_classes)}\n",
    "    y_train_cleaned = np.array([label_map[label] for label in y_train_denoised])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaned data\n",
    "class LFWDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = LFWDataset(X_train_denoised, y_train_denoised)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = 4096  \n",
    "num_classes = len(np.unique(y_train_denoised))\n",
    "hidden_size = 256  \n",
    "num_layers = 2\n",
    "model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 8.3350\n",
      "Model weights saved as 'loss_distribution.pth'\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'loss_distribution.pth')\n",
    "print(\"Model weights saved as 'loss_distribution.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.FloatTensor(X)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X)\n",
    "            probas = torch.softmax(outputs, dim=1)\n",
    "        return probas.numpy()\n",
    "\n",
    "def analyze_interpretability(model, X, feature_names=None, \n",
    "                           use_sample=True, n_background=100, \n",
    "                           n_test_samples=5,\n",
    "                           lime_samples=1):\n",
    "    \n",
    "    print(\"Starting interpretability analysis...\")\n",
    "    \n",
    "    # Initialize feature names if not provided\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    # Prepare data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Wrap model\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    \n",
    "    # SHAP Analysis\n",
    "    print(\"\\nPerforming SHAP analysis...\")\n",
    "    if use_sample:\n",
    "        print(f\"Using {n_background} background samples...\")\n",
    "        background_data = shap.kmeans(X_scaled, n_background)\n",
    "    else:\n",
    "        print(\"Using full dataset as background...\")\n",
    "        background_data = X_scaled\n",
    "    \n",
    "    explainer = shap.KernelExplainer(model_wrapper.predict_proba, background_data)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    test_samples = X_scaled[:n_test_samples]\n",
    "    shap_values = explainer.shap_values(test_samples)\n",
    "    \n",
    "    # LIME Analysis\n",
    "    print(\"\\nPerforming LIME analysis...\")\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        X_scaled,\n",
    "        feature_names=feature_names,\n",
    "        class_names=[f\"Class_{i}\" for i in range(model.fc.out_features)],\n",
    "        mode=\"classification\"\n",
    "    )\n",
    "    \n",
    "    lime_explanations = []\n",
    "    for i in range(lime_samples):\n",
    "        print(f\"Generating LIME explanation for sample {i+1}/{lime_samples}\")\n",
    "        exp = lime_explainer.explain_instance(\n",
    "            X_scaled[i],\n",
    "            model_wrapper.predict_proba,\n",
    "            num_features=10\n",
    "        )\n",
    "        lime_explanations.append(exp)\n",
    "    \n",
    "    results = {\n",
    "        'shap_values': shap_values,\n",
    "        'shap_test_samples': test_samples,\n",
    "        'lime_explanations': lime_explanations,\n",
    "        'feature_names': feature_names,\n",
    "        'background_size': background_data.shape[0]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAnalysis completed!\")\n",
    "    print(f\"SHAP values shape: {[sv.shape for sv in shap_values]}\")\n",
    "    print(f\"Number of LIME explanations: {len(lime_explanations)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_both_datasets(model, X_original, X_cleaned, feature_names=None, \n",
    "                         use_sample=True, n_background=100, \n",
    "                         n_test_samples=5, lime_samples=1):\n",
    "    print(\"Starting comparative analysis...\")\n",
    "    \n",
    "    # Analyze original data\n",
    "    print(\"\\n=== Analyzing Original Data ===\")\n",
    "    original_results = analyze_interpretability(\n",
    "        model=model,\n",
    "        X=X_original,\n",
    "        feature_names=feature_names,\n",
    "        use_sample=use_sample,\n",
    "        n_background=n_background,\n",
    "        n_test_samples=n_test_samples,\n",
    "        lime_samples=lime_samples\n",
    "    )\n",
    "    \n",
    "    # Analyze cleaned data\n",
    "    print(\"\\n=== Analyzing Cleaned Data ===\")\n",
    "    cleaned_results = analyze_interpretability(\n",
    "        model=model,\n",
    "        X=X_cleaned,\n",
    "        feature_names=feature_names,\n",
    "        use_sample=use_sample,\n",
    "        n_background=n_background,\n",
    "        n_test_samples=n_test_samples,\n",
    "        lime_samples=lime_samples\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'original': original_results,\n",
    "        'cleaned': cleaned_results\n",
    "    }\n",
    "\n",
    "def plot_interpretability_results(results):\n",
    "    \"\"\"\n",
    "    Plot SHAP and LIME results\n",
    "    \"\"\"\n",
    "    # Plot SHAP summary\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        results['shap_values'], \n",
    "        results['shap_test_samples'],\n",
    "        feature_names=results['feature_names'],\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(\"SHAP Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot LIME explanations\n",
    "    for i, exp in enumerate(results['lime_explanations']):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        exp.as_pyplot_figure()\n",
    "        plt.title(f\"LIME Explanation for Instance {i+1}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then run the analysis\n",
    "results = analyze_both_datasets(\n",
    "    model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "    X_original=X_train,\n",
    "    X_cleaned=X_train_denoised,\n",
    "    use_sample=True,\n",
    "    n_background=10,\n",
    "    n_test_samples=5,\n",
    "    lime_samples=1\n",
    ")\n",
    "\n",
    "# # For full dataset analysis:\n",
    "# results = analyze_both_datasets(\n",
    "#     model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "#     X_original=X_train,\n",
    "#     X_cleaned=X_train_denoised,\n",
    "#     use_sample=False,\n",
    "#     n_test_samples=20,\n",
    "#     lime_samples=5\n",
    "# )\n",
    "\n",
    "# Plot results for both datasets\n",
    "print(\"\\nPlotting Original Data Results:\")\n",
    "plot_interpretability_results(results['original'])\n",
    "\n",
    "print(\"\\nPlotting Cleaned Data Results:\")\n",
    "plot_interpretability_results(results['cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S-RISE (Saliency-guided Random Input Sampling for Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRISE:\n",
    "    def __init__(self, model, input_size, n_samples=1000, s=8, p1=0.5, sigma=2.0):\n",
    "        \"\"\"\n",
    "        Initialize S-RISE.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to explain\n",
    "            input_size: Size of input features\n",
    "            n_samples: Number of mask samples\n",
    "            s: Stride size\n",
    "            p1: Base probability\n",
    "            sigma: Gaussian smoothing parameter\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        # Convert input_size to 2D if it's 1D\n",
    "        if isinstance(input_size, int):\n",
    "            self.height = int(np.sqrt(input_size))\n",
    "            self.width = self.height\n",
    "            if self.height * self.width != input_size:\n",
    "                raise ValueError(f\"Input size {input_size} must be a perfect square for 2D reshaping\")\n",
    "        else:\n",
    "            self.height, self.width = input_size\n",
    "            \n",
    "        self.n_samples = n_samples\n",
    "        self.s = s\n",
    "        self.p1 = p1\n",
    "        self.sigma = sigma\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.masks = self.generate_masks()\n",
    "        \n",
    "    def gaussian_kernel(self, sigma):\n",
    "        \"\"\"Generate Gaussian kernel.\"\"\"\n",
    "        kernel_size = int(4 * sigma + 1)\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "            \n",
    "        center = kernel_size // 2\n",
    "        x, y = np.meshgrid(np.arange(kernel_size), np.arange(kernel_size))\n",
    "        kernel = np.exp(-((x - center) ** 2 + (y - center) ** 2) / (2 * sigma ** 2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        \n",
    "        return torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def gaussian_blur(self, x, sigma):\n",
    "        \"\"\"Apply Gaussian blur to tensor.\"\"\"\n",
    "        kernel = self.gaussian_kernel(sigma)\n",
    "        kernel = kernel.to(x.device)\n",
    "        channels = x.shape[1]\n",
    "        \n",
    "        # Ensure kernel size is smaller than input dimensions\n",
    "        if kernel.shape[-1] >= min(x.shape[-2:]):\n",
    "            kernel = self.gaussian_kernel(sigma/2)  # Use smaller kernel\n",
    "            \n",
    "        padding = kernel.shape[-1]//2\n",
    "        x_padded = torch.nn.functional.pad(x, (padding, padding, padding, padding), mode='reflect')\n",
    "        return torch.nn.functional.conv2d(x_padded, kernel, groups=channels)\n",
    "    \n",
    "    def generate_masks(self):\n",
    "        \"\"\"Generate random masks.\"\"\"\n",
    "        masks = []\n",
    "        for _ in range(self.n_samples):\n",
    "            # Generate 2D mask\n",
    "            h = int(np.ceil(self.height/self.s))\n",
    "            w = int(np.ceil(self.width/self.s))\n",
    "            mask = np.random.choice([0, 1], size=(h, w), p=[1-self.p1, self.p1])\n",
    "            \n",
    "            # Upsample to full size\n",
    "            mask = torch.FloatTensor(mask)\n",
    "            mask = nn.functional.interpolate(\n",
    "                mask.unsqueeze(0).unsqueeze(0),\n",
    "                size=(self.height, self.width),\n",
    "                mode='nearest'\n",
    "            )\n",
    "            \n",
    "            # Apply Gaussian smoothing\n",
    "            mask = self.gaussian_blur(mask, self.sigma).squeeze()\n",
    "            masks.append(mask)\n",
    "            \n",
    "        return torch.stack(masks).to(self.device)\n",
    "    \n",
    "    def explain(self, x, batch_size=32):\n",
    "        \"\"\"Generate saliency map for input x.\"\"\"\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "        # Convert the tensor to float32\n",
    "        x = x.float() \n",
    "\n",
    "        if len(x.shape) == 1:\n",
    "            # Reshape 1D input to 2D\n",
    "            x = x.unsqueeze(0).unsqueeze(0).view(1, 1, self.height, self.width)\n",
    "        elif len(x.shape) == 2:\n",
    "            # If it's already 2D, add the batch and channel dimensions\n",
    "            x = x.unsqueeze(0).unsqueeze(0)  # Shape becomes (1, 1, height, width)\n",
    "        elif len(x.shape) == 3:\n",
    "            # If it's 3D, assume the input shape is (channels, height, width)\n",
    "            x = x.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get original prediction\n",
    "            original_pred = self.model(x)\n",
    "            pred_class = original_pred.argmax().item()\n",
    "            \n",
    "            # Process masks in batches\n",
    "            for i in range(0, self.n_samples, batch_size):\n",
    "                batch_masks = self.masks[i:i+batch_size]\n",
    "                batch_size = len(batch_masks)\n",
    "                \n",
    "                # Apply masks\n",
    "                masked_inputs = x.repeat(batch_size, 1, 1, 1) * batch_masks.unsqueeze(1)\n",
    "                batch_preds = self.model(masked_inputs.view(batch_size, -1))\n",
    "                predictions.append(batch_preds[:, pred_class])\n",
    "                \n",
    "        # Calculate saliency map\n",
    "        predictions = torch.cat(predictions)\n",
    "        saliency_map = torch.zeros(self.height * self.width, dtype=torch.float32)\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            saliency_map += pred * self.masks[i].view(-1)\n",
    "            \n",
    "        return saliency_map.view(self.height, self.width)\n",
    "    \n",
    "    def visualize(self, x, saliency_map, save_path):\n",
    "        \"\"\"Visualize original input and saliency map.\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('Original Input')\n",
    "        \n",
    "        # Check if x is a tensor and print its shape\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            # Ensure x is reshaped and moved to CPU before converting to NumPy\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.view(1, -1)  # Ensure it's at least 2D\n",
    "            original_input = x.view(self.height, self.width).cpu().detach().numpy()\n",
    "        else:\n",
    "            raise ValueError(\"Input x must be a torch.Tensor\")\n",
    "        \n",
    "        plt.imshow(original_input, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot saliency map\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title('Saliency Map')\n",
    "        \n",
    "        # Ensure saliency_map is moved to CPU before converting to NumPy\n",
    "        if isinstance(saliency_map, torch.Tensor):\n",
    "            saliency_map_cpu = saliency_map.cpu().detach().numpy()\n",
    "        else:\n",
    "            raise ValueError(\"Saliency map must be a torch.Tensor\")\n",
    "\n",
    "        plt.imshow(saliency_map_cpu, cmap='hot')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "def compare_srise_explanations(model, X_original, X_cleaned, n_samples=3):\n",
    "    \"\"\"Compare SRISE explanations for original and cleaned data.\"\"\"\n",
    "    input_size = X_original.shape[1]\n",
    "    srise = SRISE(model, input_size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4*n_samples))\n",
    "    for i in range(n_samples):\n",
    "        # Original data\n",
    "        smap_original = srise.explain(X_original[i])\n",
    "        plt.subplot(n_samples, 2, 2*i + 1)\n",
    "        plt.title(f'Original Data - Sample {i+1}')\n",
    "        plt.imshow(smap_original.cpu().numpy(), cmap='hot')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Cleaned data\n",
    "        smap_cleaned = srise.explain(X_cleaned[i])\n",
    "        plt.subplot(n_samples, 2, 2*i + 2)\n",
    "        plt.title(f'Cleaned Data - Sample {i+1}')\n",
    "        plt.imshow(smap_cleaned.cpu().numpy(), cmap='hot')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S-RISE with your model\n",
    "srise = SRISE(\n",
    "    model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "    input_size=4096,  # Will be automatically reshaped to (64, 64)\n",
    "    n_samples=1000,\n",
    "    s=8,\n",
    "    p1=0.5,\n",
    "    sigma=2.0\n",
    ")\n",
    "\n",
    "# Generate saliency map for a sample\n",
    "sample = X_train[0]\n",
    "\n",
    "# Convert ndarray to PyTorch tensor\n",
    "if isinstance(sample, np.ndarray):\n",
    "    sample = torch.tensor(sample)\n",
    "\n",
    "# Ensure the tensor is of type float32\n",
    "sample = sample.float()\n",
    "    \n",
    "saliency_map = srise.explain(sample, batch_size=32)\n",
    "\n",
    "# Visualize results\n",
    "srise.visualize(sample, saliency_map, save_path='srise_visualization.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original and cleaned data\n",
    "def compare_srise_explanations(model, X_original, X_cleaned, n_samples=3):\n",
    "    srise = SRISE(model, X_original.shape[1])\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original data\n",
    "        original = X_original[i]\n",
    "        smap_original = srise.explain(original)\n",
    "        print(f\"\\nOriginal Data - Sample {i+1}:\")\n",
    "        if isinstance(original, np.ndarray):\n",
    "            original = torch.tensor(original)\n",
    "        original = original.float()\n",
    "        srise.visualize(original, smap_original, save_path='srise_visualization_original.png')\n",
    "        \n",
    "        # Cleaned data\n",
    "        cleaned =  X_cleaned[i]\n",
    "        smap_cleaned = srise.explain(cleaned)\n",
    "        print(f\"\\nCleaned Data - Sample {i+1}:\")\n",
    "        if isinstance(cleaned, np.ndarray):\n",
    "            cleaned = torch.tensor(cleaned)\n",
    "        cleaned = cleaned.float()\n",
    "        srise.visualize(cleaned, smap_cleaned, save_path='srise_visualization_cleaned.png')\n",
    "        \n",
    "# Run comparison\n",
    "compare_srise_explanations(\n",
    "    model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "    X_original=X_train,\n",
    "    X_cleaned=X_train_denoised,\n",
    "    n_samples=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='best_model.pth'):  # Change to .pth\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.acc_list = []\n",
    "\n",
    "    def __call__(self, val_loss, model, test_acc):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)  # Save the initial model\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.acc_list.append(test_acc)\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)  # Save the new best model\n",
    "            self.counter = 0\n",
    "            self.acc_list.clear()\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  # Save as .pth file\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "class LFWDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def co_teaching_loss(model1_loss, model2_loss, rt):\n",
    "    \"\"\"Implementation of co-teaching loss\"\"\"\n",
    "    _, model1_sm_idx = torch.topk(model1_loss, k=int(int(model1_loss.size(0)) * rt), largest=False)\n",
    "    _, model2_sm_idx = torch.topk(model2_loss, k=int(int(model2_loss.size(0)) * rt), largest=False)\n",
    "\n",
    "    model1_loss_filter = torch.zeros((model1_loss.size(0))).to(model1_loss.device)\n",
    "    model1_loss_filter[model2_sm_idx] = 1.0\n",
    "    model1_loss = (model1_loss_filter * model1_loss).sum()\n",
    "\n",
    "    model2_loss_filter = torch.zeros((model2_loss.size(0))).to(model2_loss.device)\n",
    "    model2_loss_filter[model1_sm_idx] = 1.0\n",
    "    model2_loss = (model2_loss_filter * model2_loss).sum()\n",
    "\n",
    "    return model1_loss, model2_loss\n",
    "\n",
    "def update_reduce_step(current_step, num_gradual, tau=0.5):\n",
    "    \"\"\"Update the reduction rate during training\"\"\"\n",
    "    return 1.0 - tau * min(current_step / num_gradual, 1)\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Validation step\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    return total_loss / len(val_loader), correct / total\n",
    "\n",
    "def train_coteaching_no_noise(model1, model2, train_loader, val_loader, test_loader, \n",
    "                              num_epochs=100, lr=0.001, num_gradual=10, tau=0.5, \n",
    "                              patience=7, device=\"cuda\"):\n",
    "    \n",
    "    optimizer = optim.Adam(chain(model1.parameters(), model2.parameters()), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')  # For per-sample losses\n",
    "    criterion_val = nn.CrossEntropyLoss()  # For validation (mean reduction)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path='co_teaching.pth')  \n",
    "    \n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    model1 = model1.to(device)\n",
    "    model2 = model2.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        rt = update_reduce_step(current_step=epoch, num_gradual=num_gradual, tau=tau)\n",
    "        \n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        train_acc = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass with all data (no noise filtering)\n",
    "            out1 = model1(x)\n",
    "            out2 = model2(x)\n",
    "\n",
    "            model1_loss = criterion(out1, y)\n",
    "            model2_loss = criterion(out2, y)\n",
    "            \n",
    "            model1_loss, model2_loss = co_teaching_loss(\n",
    "                model1_loss=model1_loss,\n",
    "                model2_loss=model2_loss,\n",
    "                rt=rt\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model1_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model2_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, pred1 = torch.max(out1.data, 1)\n",
    "            train_acc += (pred1 == y).sum().item()\n",
    "            train_total += y.size(0)\n",
    "        \n",
    "        train_accuracy = train_acc / train_total\n",
    "        val_loss, val_acc = validate_model(model1, val_loader, criterion_val, device)\n",
    "        \n",
    "        model1.eval()\n",
    "        test_acc = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model1(x)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += y.size(0)\n",
    "                test_acc += (predicted == y).sum().item()\n",
    "        \n",
    "        test_accuracy = test_acc / test_total\n",
    "        early_stopping(val_loss, model1, test_accuracy)\n",
    "        \n",
    "        train_acc_list.append(train_accuracy)\n",
    "        test_acc_list.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, '\n",
    "              f'Test Acc: {test_accuracy:.4f}, Reduce Rate: {rt:.4f}')\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    model1.load_state_dict(torch.load('co_teaching.pth'))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_acc_list, label='Training Accuracy')\n",
    "    plt.plot(test_acc_list, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model1, model2\n",
    "\n",
    "def train_lfw_without_noise(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                            input_size, hidden_size, num_layers, num_classes):\n",
    "    train_dataset = LFWDataset(X_train, y_train)\n",
    "    val_dataset = LFWDataset(X_val, y_val)\n",
    "    test_dataset = LFWDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model1 = FaceRecognitionRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "    model2 = FaceRecognitionRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "    \n",
    "    trained_model1, trained_model2 = train_coteaching_no_noise(\n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=5,\n",
    "        lr=0.001,\n",
    "        num_gradual=10,\n",
    "        tau=0.5,\n",
    "        patience=7,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    return trained_model1, trained_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 8.320193). Saving model ...\n",
      "Epoch [1/5], Train Acc: 0.0004, Val Loss: 8.3202, Val Acc: 0.0002, Test Acc: 0.0002, Reduce Rate: 1.0000\n",
      "Validation loss decreased (8.320193 --> 8.097259). Saving model ...\n",
      "Epoch [2/5], Train Acc: 0.0002, Val Loss: 8.0973, Val Acc: 0.0002, Test Acc: 0.0010, Reduce Rate: 0.9500\n",
      "Validation loss decreased (8.097259 --> 7.808554). Saving model ...\n",
      "Epoch [3/5], Train Acc: 0.0003, Val Loss: 7.8086, Val Acc: 0.0020, Test Acc: 0.0015, Reduce Rate: 0.9000\n",
      "Validation loss decreased (7.808554 --> 7.582337). Saving model ...\n",
      "Epoch [4/5], Train Acc: 0.0009, Val Loss: 7.5823, Val Acc: 0.0039, Test Acc: 0.0029, Reduce Rate: 0.8500\n",
      "Validation loss decreased (7.582337 --> 7.409552). Saving model ...\n",
      "Epoch [5/5], Train Acc: 0.0021, Val Loss: 7.4096, Val Acc: 0.0054, Test Acc: 0.0061, Reduce Rate: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anvesha\\AppData\\Local\\Temp\\ipykernel_25540\\2270260355.py:168: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1.load_state_dict(torch.load('co_teaching.pth'))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model parameters\n",
    "input_size = 128  # Size of each input sample\n",
    "hidden_size = 64  # Number of hidden units\n",
    "num_layers = 2    # Number of layers in RNN\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Train the model using co-teaching\n",
    "trained_model1, trained_model2 = train_lfw_without_noise(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    input_size, hidden_size, num_layers, num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.FloatTensor(X)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X)\n",
    "            probas = torch.softmax(outputs, dim=1)\n",
    "        return probas.numpy()\n",
    "\n",
    "def analyze_interpretability(model, X, feature_names=None, \n",
    "                           use_sample=True, n_background=100, \n",
    "                           n_test_samples=5,\n",
    "                           lime_samples=1):\n",
    "    \n",
    "    print(\"Starting interpretability analysis...\")\n",
    "    \n",
    "    # Initialize feature names if not provided\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    # Prepare data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Wrap model\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    \n",
    "    # SHAP Analysis\n",
    "    print(\"\\nPerforming SHAP analysis...\")\n",
    "    if use_sample:\n",
    "        print(f\"Using {n_background} background samples...\")\n",
    "        background_data = shap.kmeans(X_scaled, n_background)\n",
    "    else:\n",
    "        print(\"Using full dataset as background...\")\n",
    "        background_data = X_scaled\n",
    "    \n",
    "    explainer = shap.KernelExplainer(model_wrapper.predict_proba, background_data)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    test_samples = X_scaled[:n_test_samples]\n",
    "    shap_values = explainer.shap_values(test_samples)\n",
    "    \n",
    "    # LIME Analysis\n",
    "    print(\"\\nPerforming LIME analysis...\")\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        X_scaled,\n",
    "        feature_names=feature_names,\n",
    "        class_names=[f\"Class_{i}\" for i in range(model.fc.out_features)],\n",
    "        mode=\"classification\"\n",
    "    )\n",
    "    \n",
    "    lime_explanations = []\n",
    "    for i in range(lime_samples):\n",
    "        print(f\"Generating LIME explanation for sample {i+1}/{lime_samples}\")\n",
    "        exp = lime_explainer.explain_instance(\n",
    "            X_scaled[i],\n",
    "            model_wrapper.predict_proba,\n",
    "            num_features=10\n",
    "        )\n",
    "        lime_explanations.append(exp)\n",
    "    \n",
    "    results = {\n",
    "        'shap_values': shap_values,\n",
    "        'shap_test_samples': test_samples,\n",
    "        'lime_explanations': lime_explanations,\n",
    "        'feature_names': feature_names,\n",
    "        'background_size': background_data.shape[0]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAnalysis completed!\")\n",
    "    print(f\"SHAP values shape: {[sv.shape for sv in shap_values]}\")\n",
    "    print(f\"Number of LIME explanations: {len(lime_explanations)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_both_datasets(model, X_original, X_cleaned, feature_names=None, \n",
    "                         use_sample=True, n_background=100, \n",
    "                         n_test_samples=5, lime_samples=1):\n",
    "    print(\"Starting comparative analysis...\")\n",
    "    \n",
    "    # Analyze original data\n",
    "    print(\"\\n=== Analyzing Original Data ===\")\n",
    "    original_results = analyze_interpretability(\n",
    "        model=model,\n",
    "        X=X_original,\n",
    "        feature_names=feature_names,\n",
    "        use_sample=use_sample,\n",
    "        n_background=n_background,\n",
    "        n_test_samples=n_test_samples,\n",
    "        lime_samples=lime_samples\n",
    "    )\n",
    "    \n",
    "    # Analyze cleaned data\n",
    "    print(\"\\n=== Analyzing Cleaned Data ===\")\n",
    "    cleaned_results = analyze_interpretability(\n",
    "        model=model,\n",
    "        X=X_cleaned,\n",
    "        feature_names=feature_names,\n",
    "        use_sample=use_sample,\n",
    "        n_background=n_background,\n",
    "        n_test_samples=n_test_samples,\n",
    "        lime_samples=lime_samples\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'original': original_results,\n",
    "        'cleaned': cleaned_results\n",
    "    }\n",
    "\n",
    "def plot_interpretability_results(results):\n",
    "    \"\"\"\n",
    "    Plot SHAP and LIME results\n",
    "    \"\"\"\n",
    "    # Plot SHAP summary\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        results['shap_values'], \n",
    "        results['shap_test_samples'],\n",
    "        feature_names=results['feature_names'],\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(\"SHAP Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot LIME explanations\n",
    "    for i, exp in enumerate(results['lime_explanations']):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        exp.as_pyplot_figure()\n",
    "        plt.title(f\"LIME Explanation for Instance {i+1}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparative analysis...\n",
      "\n",
      "=== Analyzing Original Data ===\n",
      "Starting interpretability analysis...\n",
      "\n",
      "Performing SHAP analysis...\n",
      "Using 10 background samples...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Then run the analysis\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_both_datasets\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFaceRecognitionRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_original\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_cleaned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_cleaned\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_background\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_test_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlime_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# # For full dataset analysis:\u001b[39;00m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# results = analyze_both_datasets(\u001b[39;00m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#     model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     21\u001b[0m \n",
      "\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Plot results for both datasets\u001b[39;00m\n",
      "\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlotting Original Data Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[50], line 94\u001b[0m, in \u001b[0;36manalyze_both_datasets\u001b[1;34m(model, X_original, X_cleaned, feature_names, use_sample, n_background, n_test_samples, lime_samples)\u001b[0m\n",
      "\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Analyze original data\u001b[39;00m\n",
      "\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Analyzing Original Data ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m---> 94\u001b[0m original_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_interpretability\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_original\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sample\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_background\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_background\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_test_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_test_samples\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlime_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlime_samples\u001b[49m\n",
      "\u001b[0;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Analyze cleaned data\u001b[39;00m\n",
      "\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Analyzing Cleaned Data ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[50], line 43\u001b[0m, in \u001b[0;36manalyze_interpretability\u001b[1;34m(model, X, feature_names, use_sample, n_background, n_test_samples, lime_samples)\u001b[0m\n",
      "\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_sample:\n",
      "\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_background\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m background samples...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m---> 43\u001b[0m     background_data \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_background\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing full dataset as background...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Anvesha\\miniconda3\\envs\\myenv\\lib\\site-packages\\shap\\utils\\_legacy.py:41\u001b[0m, in \u001b[0;36mkmeans\u001b[1;34m(X, k, round_values)\u001b[0m\n",
      "\u001b[0;32m     38\u001b[0m X \u001b[38;5;241m=\u001b[39m imp\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Specify `n_init` for consistent behaviour between sklearn versions\u001b[39;00m\n",
      "\u001b[1;32m---> 41\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m round_values:\n",
      "\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Anvesha\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n",
      "\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n",
      "\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n",
      "\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n",
      "\u001b[0;32m   1471\u001b[0m     )\n",
      "\u001b[0;32m   1472\u001b[0m ):\n",
      "\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Anvesha\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1519\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n",
      "\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m   1518\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n",
      "\u001b[1;32m-> 1519\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_single\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenters_init\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tol\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1529\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n",
      "\u001b[0;32m   1530\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n",
      "\u001b[0;32m   1531\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n",
      "\u001b[0;32m   1532\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n",
      "\u001b[0;32m   1533\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n",
      "\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_inertia \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n",
      "\u001b[0;32m   1535\u001b[0m     inertia \u001b[38;5;241m<\u001b[39m best_inertia\n",
      "\u001b[0;32m   1536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n",
      "\u001b[0;32m   1537\u001b[0m ):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Anvesha\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\parallel.py:162\u001b[0m, in \u001b[0;36m_threadpool_controller_decorator.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    160\u001b[0m controller \u001b[38;5;241m=\u001b[39m _get_threadpool_controller()\n",
      "\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api):\n",
      "\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Anvesha\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:707\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[1;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n",
      "\u001b[0;32m    704\u001b[0m strict_convergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n",
      "\u001b[1;32m--> 707\u001b[0m     \u001b[43mlloyd_iter\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenters_new\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_in_clusters\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_shift\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;32m    719\u001b[0m         inertia \u001b[38;5;241m=\u001b[39m _inertia(X, sample_weight, centers, labels, n_threads)\n",
      "\n",
      "File \u001b[1;32m_k_means_lloyd.pyx:160\u001b[0m, in \u001b[0;36msklearn.cluster._k_means_lloyd.lloyd_iter_chunked_dense\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32m_k_means_common.pyx:180\u001b[0m, in \u001b[0;36msklearn.cluster._k_means_common._relocate_empty_clusters_dense\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Anvesha\\miniconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(condition, x, y)\u001b[0m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n",
      "\u001b[0;32m    258\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    341\u001b[0m \n",
      "\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n",
      "\u001b[1;32m--> 346\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n",
      "\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n",
      "\u001b[0;32m    350\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n",
      "\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Then run the analysis\n",
    "results = analyze_both_datasets(\n",
    "    model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "    X_original=X_train,\n",
    "    X_cleaned=X_train_denoised,\n",
    "    use_sample=True,\n",
    "    n_background=10,\n",
    "    n_test_samples=5,\n",
    "    lime_samples=1\n",
    ")\n",
    "\n",
    "# # For full dataset analysis:\n",
    "# results = analyze_both_datasets(\n",
    "#     model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "#     X_original=X_train,\n",
    "#     X_cleaned=X_train_denoised,\n",
    "#     use_sample=False,\n",
    "#     n_test_samples=20,\n",
    "#     lime_samples=5\n",
    "# )\n",
    "\n",
    "# Plot results for both datasets\n",
    "print(\"\\nPlotting Original Data Results:\")\n",
    "plot_interpretability_results(results['original'])\n",
    "\n",
    "print(\"\\nPlotting Cleaned Data Results:\")\n",
    "plot_interpretability_results(results['cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S-RISE (Saliency-guided Random Input Sampling for Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRISE:\n",
    "    def __init__(self, model, input_size, n_samples=1000, s=8, p1=0.5, sigma=2.0):\n",
    "        \"\"\"\n",
    "        Initialize S-RISE.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to explain\n",
    "            input_size: Size of input features\n",
    "            n_samples: Number of mask samples\n",
    "            s: Stride size\n",
    "            p1: Base probability\n",
    "            sigma: Gaussian smoothing parameter\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        # Convert input_size to 2D if it's 1D\n",
    "        if isinstance(input_size, int):\n",
    "            self.height = int(np.sqrt(input_size))\n",
    "            self.width = self.height\n",
    "            if self.height * self.width != input_size:\n",
    "                raise ValueError(f\"Input size {input_size} must be a perfect square for 2D reshaping\")\n",
    "        else:\n",
    "            self.height, self.width = input_size\n",
    "            \n",
    "        self.n_samples = n_samples\n",
    "        self.s = s\n",
    "        self.p1 = p1\n",
    "        self.sigma = sigma\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.masks = self.generate_masks()\n",
    "        \n",
    "    def gaussian_kernel(self, sigma):\n",
    "        \"\"\"Generate Gaussian kernel.\"\"\"\n",
    "        kernel_size = int(4 * sigma + 1)\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "            \n",
    "        center = kernel_size // 2\n",
    "        x, y = np.meshgrid(np.arange(kernel_size), np.arange(kernel_size))\n",
    "        kernel = np.exp(-((x - center) ** 2 + (y - center) ** 2) / (2 * sigma ** 2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        \n",
    "        return torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def gaussian_blur(self, x, sigma):\n",
    "        \"\"\"Apply Gaussian blur to tensor.\"\"\"\n",
    "        kernel = self.gaussian_kernel(sigma)\n",
    "        kernel = kernel.to(x.device)\n",
    "        channels = x.shape[1]\n",
    "        \n",
    "        # Ensure kernel size is smaller than input dimensions\n",
    "        if kernel.shape[-1] >= min(x.shape[-2:]):\n",
    "            kernel = self.gaussian_kernel(sigma/2)  # Use smaller kernel\n",
    "            \n",
    "        padding = kernel.shape[-1]//2\n",
    "        x_padded = torch.nn.functional.pad(x, (padding, padding, padding, padding), mode='reflect')\n",
    "        return torch.nn.functional.conv2d(x_padded, kernel, groups=channels)\n",
    "    \n",
    "    def generate_masks(self):\n",
    "        \"\"\"Generate random masks.\"\"\"\n",
    "        masks = []\n",
    "        for _ in range(self.n_samples):\n",
    "            # Generate 2D mask\n",
    "            h = int(np.ceil(self.height/self.s))\n",
    "            w = int(np.ceil(self.width/self.s))\n",
    "            mask = np.random.choice([0, 1], size=(h, w), p=[1-self.p1, self.p1])\n",
    "            \n",
    "            # Upsample to full size\n",
    "            mask = torch.FloatTensor(mask)\n",
    "            mask = nn.functional.interpolate(\n",
    "                mask.unsqueeze(0).unsqueeze(0),\n",
    "                size=(self.height, self.width),\n",
    "                mode='nearest'\n",
    "            )\n",
    "            \n",
    "            # Apply Gaussian smoothing\n",
    "            mask = self.gaussian_blur(mask, self.sigma).squeeze()\n",
    "            masks.append(mask)\n",
    "            \n",
    "        return torch.stack(masks).to(self.device)\n",
    "    \n",
    "    def explain(self, x, batch_size=32):\n",
    "        \"\"\"Generate saliency map for input x.\"\"\"\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "        # Convert the tensor to float32\n",
    "        x = x.float() \n",
    "\n",
    "        if len(x.shape) == 1:\n",
    "            # Reshape 1D input to 2D\n",
    "            x = x.unsqueeze(0).unsqueeze(0).view(1, 1, self.height, self.width)\n",
    "        elif len(x.shape) == 2:\n",
    "            # If it's already 2D, add the batch and channel dimensions\n",
    "            x = x.unsqueeze(0).unsqueeze(0)  # Shape becomes (1, 1, height, width)\n",
    "        elif len(x.shape) == 3:\n",
    "            # If it's 3D, assume the input shape is (channels, height, width)\n",
    "            x = x.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get original prediction\n",
    "            original_pred = self.model(x)\n",
    "            pred_class = original_pred.argmax().item()\n",
    "            \n",
    "            # Process masks in batches\n",
    "            for i in range(0, self.n_samples, batch_size):\n",
    "                batch_masks = self.masks[i:i+batch_size]\n",
    "                batch_size = len(batch_masks)\n",
    "                \n",
    "                # Apply masks\n",
    "                masked_inputs = x.repeat(batch_size, 1, 1, 1) * batch_masks.unsqueeze(1)\n",
    "                batch_preds = self.model(masked_inputs.view(batch_size, -1))\n",
    "                predictions.append(batch_preds[:, pred_class])\n",
    "                \n",
    "        # Calculate saliency map\n",
    "        predictions = torch.cat(predictions)\n",
    "        saliency_map = torch.zeros(self.height * self.width, dtype=torch.float32)\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            saliency_map += pred * self.masks[i].view(-1)\n",
    "            \n",
    "        return saliency_map.view(self.height, self.width)\n",
    "    \n",
    "    def visualize(self, x, saliency_map, save_path):\n",
    "        \"\"\"Visualize original input and saliency map.\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('Original Input')\n",
    "        \n",
    "        # Check if x is a tensor and print its shape\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            # Ensure x is reshaped and moved to CPU before converting to NumPy\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.view(1, -1)  # Ensure it's at least 2D\n",
    "            original_input = x.view(self.height, self.width).cpu().detach().numpy()\n",
    "        else:\n",
    "            raise ValueError(\"Input x must be a torch.Tensor\")\n",
    "        \n",
    "        plt.imshow(original_input, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot saliency map\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title('Saliency Map')\n",
    "        \n",
    "        # Ensure saliency_map is moved to CPU before converting to NumPy\n",
    "        if isinstance(saliency_map, torch.Tensor):\n",
    "            saliency_map_cpu = saliency_map.cpu().detach().numpy()\n",
    "        else:\n",
    "            raise ValueError(\"Saliency map must be a torch.Tensor\")\n",
    "\n",
    "        plt.imshow(saliency_map_cpu, cmap='hot')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "def compare_srise_explanations(model, X_original, X_cleaned, n_samples=3):\n",
    "    \"\"\"Compare SRISE explanations for original and cleaned data.\"\"\"\n",
    "    input_size = X_original.shape[1]\n",
    "    srise = SRISE(model, input_size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4*n_samples))\n",
    "    for i in range(n_samples):\n",
    "        # Original data\n",
    "        smap_original = srise.explain(X_original[i])\n",
    "        plt.subplot(n_samples, 2, 2*i + 1)\n",
    "        plt.title(f'Original Data - Sample {i+1}')\n",
    "        plt.imshow(smap_original.cpu().numpy(), cmap='hot')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Cleaned data\n",
    "        smap_cleaned = srise.explain(X_cleaned[i])\n",
    "        plt.subplot(n_samples, 2, 2*i + 2)\n",
    "        plt.title(f'Cleaned Data - Sample {i+1}')\n",
    "        plt.imshow(smap_cleaned.cpu().numpy(), cmap='hot')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S-RISE with your model\n",
    "srise = SRISE(\n",
    "    model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "    input_size=4096,  # Will be automatically reshaped to (64, 64)\n",
    "    n_samples=1000,\n",
    "    s=8,\n",
    "    p1=0.5,\n",
    "    sigma=2.0\n",
    ")\n",
    "\n",
    "# Generate saliency map for a sample\n",
    "sample = X_train[0]\n",
    "\n",
    "# Convert ndarray to PyTorch tensor\n",
    "if isinstance(sample, np.ndarray):\n",
    "    sample = torch.tensor(sample)\n",
    "\n",
    "# Ensure the tensor is of type float32\n",
    "sample = sample.float()\n",
    "    \n",
    "saliency_map = srise.explain(sample, batch_size=32)\n",
    "\n",
    "# Visualize results\n",
    "srise.visualize(sample, saliency_map, save_path='srise_visualization.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original and cleaned data\n",
    "def compare_srise_explanations(model, X_original, X_cleaned, n_samples=3):\n",
    "    srise = SRISE(model, X_original.shape[1])\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original data\n",
    "        original = X_original[i]\n",
    "        smap_original = srise.explain(original)\n",
    "        print(f\"\\nOriginal Data - Sample {i+1}:\")\n",
    "        if isinstance(original, np.ndarray):\n",
    "            original = torch.tensor(original)\n",
    "        original = original.float()\n",
    "        srise.visualize(original, smap_original, save_path='srise_visualization_original.png')\n",
    "        \n",
    "        # Cleaned data\n",
    "        cleaned =  X_cleaned[i]\n",
    "        smap_cleaned = srise.explain(cleaned)\n",
    "        print(f\"\\nCleaned Data - Sample {i+1}:\")\n",
    "        if isinstance(cleaned, np.ndarray):\n",
    "            cleaned = torch.tensor(cleaned)\n",
    "        cleaned = cleaned.float()\n",
    "        srise.visualize(cleaned, smap_cleaned, save_path='srise_visualization_cleaned.png')\n",
    "        \n",
    "# Run comparison\n",
    "compare_srise_explanations(\n",
    "    model = FaceRecognitionRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes),\n",
    "    X_original=X_train,\n",
    "    X_cleaned=X_train_denoised,\n",
    "    n_samples=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
